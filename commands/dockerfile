
# FROM - Allow us to build our image up on another base image. sidenote: we could build a Docker image from scratch,
# but we always want some kind of operating system layer in there, some kind of other tool which our code needs
FROM node:12

# WORKDIR: Working directory that tells Docker that all the comands will be executed from inside that folder, so app folder 
# (in our case)
WORKDIR /app

# Since all layers changes after the changing layer, we add the package.json here, so when we'll change our source code
# (when we copy after npm install) only the subsequent instructions we'll re-execute, and then the docker we'll not run
# npm install when the source code change, since there is no need, because our dependencies not changed
COPY package.json /app

# RUN - We telling Docker after copying, to run npm install on our image for all the dependencies 
RUN npm install

# COPY - We tell Docker which files that live here on our local machine, should go into the image, and we specify two path here

# First path: path of the outside container/image where the files leave that should be copied into the image, and a dot 
# means that is the same folder that contain the Dockerfile. So, it tell Docker that all 
# folders, sub folders, files should be copie into the image

# Second path: the path inside of the image where those files should be stored. In our case, it's the sub folder app
# where all our folders/subfolders, files where the Dockerfile, so all of them will copied to /app. And if this folder not
# exist it will be created
# COPY . /app
COPY . .

# Build Time Arguments - With those we can plug if different values into our Dockerfile or into our image when we build
# that image without having to hard code these values into the Dockerfile, and this arguments can be use only in the 
# Dockerfile on the instructions, except to the CMD [...], because it's runtime command, so it's executed when the
# container starts. So we set a dynamic argument which then is set as a default value (DEFAULT_PORT) for the dynamic
# evironment variable. Important to set this here and not at the beginning, since these instuctions add layers
# to Dockerfile, so when something changes about them, all susbequent layers are rebuilt and re-executed, and 
# we don't wanna exeute npm install and all the other instructions again

ARG DEFAULT_PORT=80

# We add the ENV instruction and announce of the environment variable as a PORT with default value of our argument -
# DEFAULT_PORT. So the DEFAULT_PORT will be available on the entire application, but we still can override this when we 
# set another environment variable/port when we use commands in creating image/container 
ENV PORT=$DEFAULT_PORT

# We expose the port from our isolated conatiner to our local machine, but actually this more instruction for 
# documentation purposes, so we don't need this for run our container

# $PORT - our environment variable - docker recognize it by a dolar sign
EXPOSE $PORT

# Anonymous Volume
# The path inside the container which should be mapped to some folder outside of the container and where data
# should survive if we'll delete the container. In this case it will not survive if the container will delete,
# because it's anonymous volume (as long as we created the container with --rm command)
VOLUME [ "/app/feedback" ]

# VOLUME [ "/app/node_modules" ]

# So our node server will run when a container is started based on the image, and not the imaged itself.
# The difference between RUN and CMD, that RUN will setting up our image, but CMD will activate it on the container based
# from our image
# CMD [ "node", "server.js" ]

CMD [ "npm", "start" ]


COMMANDS

# docker build . => With the dot we tell Docker that the Dockerfile will be in the same folder as we're running this command

# -d : Run the container in detached mode - i.e. output printed by the container is not visible,
# the command prompt / terminal does NOT wait for the container to stop

# The -t flag allows us to assign a name to the image, which makes it easier to refer to when running

# CHECKING ERROR/S

# We looking in the output of the container to see what went wrong
# docker logs feedback-app (feedback-app -containers name)


DELETE IMAGES & CONTAINERS - *Sidenote* We can only remove images if we remove their containers first

# docker container prune - Allow us to delete all our stoped containers
# docker rm - Allow us to delete specific or multiple containers with their names

# docker images - Allow to see all our images

# docker image prune - Allow us to remove all our images which have not tags.
# docker image prune -a - Allow us to remove all unused images including images with the tags
# docker rmi - Allow us to remove specific or multiple images with their names

# docker system prune -a => Allow us to clean up unused images, so it's clear everything from the docker system 

# --rm - Create container and remove it automatically when it stop (more details on docker run --help)
# example: docker run -p 3000:80 -d --rm 05c377af849a

# docker image inspect 05c377af849a - Give us information about the image


ENTERING INTERACTIVE MODE 

# In this example we illustrate a case where we only want to run software/app and not run it as a web server - so, we 
# want to be in attached state, but in listening mode and also to be able input something, so interact with our container

# - i - t / -it => With the combining of the i and the t flags we'll able to be in listening mode, and also to input 
# something when we run our container. So the container will listen for our input, and we'll also get a terminal exposed 
# by the container, which is the device where we enter the input (for more details on -i or -t flags, check docker run 
# --help) docker run -it f5396224a0d1

# If we restarting the container and we want to listen mode and also to be able to input something mode, we'll use -a -i.

# So the difference between the -it/-i -t and the -a -i, is the -it for when we creating/running our container, and -a -i 
# when we restart our container (for more details on -a or -i flags, check docker start --help)
# docker start -a -i relaxed_grothendieck


# CP COMMAND - allow us to copy files of folders into running container or out of a running container to our local host machine

# Here we copy everything in the dummy folder from our local host machine to this container to test folder (if it not 
# exist, it will created)
# docker cp dummy/. youthful_archimedes:/test

# Here we copy from the test folder in our container to the dummy folder in our local host machine 
# docker cp youthful_archimedes:/test dummy

# Here we copy from the test folder in our container only the test.txt file to the dummy folder in our local host machine 
# docker cp youthful_archimedes:/test/test.txt dummy


NAMING & TAGGING CONTAINERS AND IMAGES

# Here we give a name for container
# docker run -p 3000:80 -d --rm --name goalsapp 74da73ce6927 (for more details check docker run --help)

# In image, the image tags(name & tag combination) consists of two parts - the actual name (repository), and the tag. 
# so, name:tag
# So with the name we set general name, and with the tag we define a more specialized version of that image 
# For example: node:12

# We build our image and give it name of goals, and tag latest, to indicate that this is latest version of the goalsapp
# The -t flag allows us to assign a name to the image, which makes it easier to refer to when running
# docker build -t goals:latest . (for more details check docker build --help) 

# We run and give the container name 'goalsapp' and plug in the image with the tag 'goals:latest' instead the image ID
# docker run -p 3000:80 -d --rm --name goalsapp goals:latest

PUSHING IMAGES TO DOCKERHUB 

# *Sidenote* - if we push/pull to a private registry (other provider), we need to inlude the host, so the url of the provider
# for example - HOST:NAME

# We need to verify that this is our account for pushing images to dockerhub. 
# login (we can also logout after)

# For pushing image to dockerhub we need to create reopository and give our image the name that we set in dockerhub
# for example: urielpa/node-hello-world:tagname

# First way - create new image with that name
# docker build -t urielpa/node-hello-world .

# Second way - we reuse that image that we have already and renaming it for retaggin images
# *Sidenote* when we renaming an image, we create a clone of the old image, so the old image not deleted

# docker tag node-demo:latest urielpa/node-hello-world(:latest)
# node-demo:latest - old name, urielpa/node-hello-world - new name (we can also allocate it a tag)

# docker push urielpa/node-hello-world(:tagname)


PULLING & USING SHARED IMAGES THROUGH DOCKERHUB

# *Sidenote* when we pull, we always pull the latest image of that name from your container registry.
# So if in the meantime we rebuild the image, and push an updated imaged to the registry,
# the next time we'll execute docker pull, we'll fetch that latest image,
# but if we have an image locally because we pulled or run it before, if we'll run a container again based on an image,
# docker run will not check if the image that we have locally on our system is the latest version of that image.
# So if the meantime we update the image and pushed it again to dockerhub and we using 'run docker' there after,
# we'll not get the latest updated image. So we need to manually 'run ducker pull' and then the image name first
# to ensure that we have the latest image version, and then execute 'docker run again'

# docker pull urielpa/node-hello-world   

EXAMPLES OF CREATEING IMAGES & CONTAINERS (WITH VOLUMES)

# We create image with the name feedback-node
# docker build -t feedback-node .

# We run container on port 3000 on our local machine in detached mode with containr's name feedback-app based
# feedback-node image, and after that it will be rmoved
# docker run -p 3000:80 -d --name feedback-app --rm feedback-node

# docker run -d -p 3000:80 --rm --name feedback-app feedback-node:volumes


VOLUMES COMMANDS

# docker volumes ls - give us the list of the volumes   


# Volumes - folders on our host machine in the hard drive, which available into the containers, so the data in our 
# machine it exactly like our data in the folder in the container that we defined. So if the data changes in one place, 
# it will change in both, and so the information will persist even if the container will deleted, because it will stay 
# on our machine

# ANONYMOUS VOLUMES VS NAMED VOLUMES

# Anonymous Volume - anonymous volume it attached to specific container. So, if the container will delete, 
# the volume will not survive, and it will erase
# example of anonymous volume: VOLUME [ "/app/feedback" ]

# Named volume - volume that can persist from container to a new container even if his previous container deleted


CREATING ANONYMOUS VOLUMES & NAMED VOLUMES & DLEETING THEM

# *Sidenote* if we're not adding --rm when we creating our container with anonymous volume, we'll need to delete the  
# volume and also the container, but if we adding the --rm, it will erase both with the container

# Example to create image with a name and volume as a tag
# docker build -t feedback-node:volumes .

# *Sidenote* important that when we creating new container and want to connect to volume, that it will be the same name
# of the volume that we named and created

# Example to create container based imaged with named volume
# docker run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback feedback-node:volumes

# Delete specific volume - docker volume rm feedback (feedback - volume name)/e329b312edf7c0e4ddf6bb6a1(volume id)

# Delete all volumes - docker volume prune 


BIND MOUNTS
# *Sidenote* - Bind Mount is not 

# Bind Mounts - It's similar to volume, but the difference that we're managed the path and the folder of our app in our 
# local machine, and not Docker like with volume that we don't know where it locate. Also the code will not copied from  
# COPY . . snapshot, but instead from the bind mount, so that folder of our app that we specify for that, and his big 
# advantage it's that we can persist editable data with him - so code that we changing in live within the container will 
# save without creating new image or new container, because whenever that the code inside the folder of our app that we 
# specified will change, it will reflected from it


# We setting/creating a Bind Mount

# First way:

# "%cd%" => The absolute path to the folder in our local machine where we are, and where we have all the source code
#  in our case. and this code would go to the mapped folder, in our case this is the app folder (the working direcotry) 
# in the container

# "%cd%":/app => Our root/working direcotry in the container where we have all the source code, so the mapped folder,
# the app folder that we specified as a working directory, because we wanna control the entire app folder, so our code 
# will change and reflected on our local machine in a live way. So basically we copy our main folder to the main folder 
# (app) in the container and so if we change code on the local machine, it will also change in the container and so any 
# change made on the local machine will automatically be visible in the container as well.

# Second way:

# ${PWD} => equal to "%cd%"
# ${PWD}:/app  => equal to "%cd%":/app 

# Work only from the powershell 
# docker run -d -p 3000:80 (--rm) --name f-app -v feedback:/app/feedback -v ${PWD}:/app feedback-node:volumes
# docker run -d -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "%cd%":/app feedback-node:volumes


COMBINING & MERGING DIFFERENT VOLUMES - Creating named volume & bind mount & anonymous volume


# -v /app/node_modules - anonymous volume because it has no name, and we can create it also like this: 
# VOLUME [ "/app/node_modules" ], but we'll need to create an new image, so we're not using this way.
# So, essentially we overwrite our bind mount where we're copied our source code from our local machine 
# to the root/working directory (app) in the container without the dependencies (because that we have no 
# dependencies in our local machine) with anonymous volume with longer internal/specific path, with the 
# source code from the container, so we overwrite it with the return first snapshot where we had the node  
# modules in the container, when we created the image

# docker run -d -p 3000:80 --name feedback-app -v feedback:/app/feedback -v "%cd%":/app -v /app/node_modules  feedback-node:volumes


# ADJUSTMENT FOR NODEMON TO NODEJS

# We needed to add -L in the scripts for nodemon work in windows because we using WSL 2
# For more details check in: https://github.com/remy/nodemon#application-isnt-restarting

# "scripts": {
#     "start": "nodemon -L server.js"
#   },


ADDING READ ONLY VOLUMES & OVERWRITING AGAIN WITH ANONYMOUS VOLUME

# The idea here, is that we want to restrict our container to read only, but still keep the permission to write
# in to feedback and temp folders, because data will go/change there, since all other changes will come from our
# host machine to the container through the bind mount

# -v "%cd%":/app:ro => ro (read only) - This ensures that from the container docker only will get read permission 
# and not write.

# -v /app/temp => We specify another anonymous volume that more specific and longer from "%cd%":/app,
# folder because we wanna overrides the main bind mount volume ("%cd%":/app), and give permission to write in to 
# the temp folder from the container, since data changing there. So we stick to one that created during the image 
# building, so the temp folder that already exist in the container. Also because it's anonymous volume, all the 
# data will erase, but we good with that, since the temp folder was originally intended for temporary files.

# -v feedback:/app/feedback => In the line below we explain about this volume, but we did not added it here,
# it already exists. We only explain why we didn't need to do anything for give write permission to write in to 
# the feedback folder.
# In the case of feedback folder we're already good, since we have already specific and longer volume that overrides
# our main bind mount volume ("%cd%":/app), the named volume that already exist -v feedback:/app/feedback.
# So because it more longer it will overrides our bind mount. So we don't need to adding nothing, we're good

# In short, we create container with read only permission, except from to write in to the feedback and the temp 
# folders, because we do need to get write permissions when we write in to these folders.
# docker run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback -v "%cd%":/app:ro -v /app/temp -v /app/node_modules feedback-node:volumes 



GENERAL INFORMATION & COMMANDS ON VOLUMES & Managing Docker Volumes

# docker volume --help

# docker volume ls - Lists of all the currently active volumes

# So here we have 3 volumes: -v feedback:/app/feedback, -v /app/temp, -v /app/node_modules, the bind mount is not count, 
# because is not managed by Docker, but we.
# docker run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback -v "%cd%":/app:ro -v /app/temp -v /app/node_modules feedback-node:volumes 


# Example to create a volume independently without also create container. So if we want, we can create a container
# and then connect to a named volume that exist, so as we used until now, but also to create a new one when we 
# create a container 
# docker volume create feedback-files

# docker volume inspect feedback - Give us some informatin about the volume

# We need to remember that if we delete a named volume, all his data also delete. So in our case, if we'll delete the
# feedback volume, all our records in the feedback folder will delete, and when we'll create new container, it will
# no longer exist
# docker volume rm feedback - delete this volume or what that we will specify here

# docker volume prune - delete all the volumes that we have


# 59. USING "COPY" VS BIND MOUNTS 

# The reason that we using the COPY snapshot and not just the bind mount (since the bind mount overwritten it anyways),
# is because we'll use it only during development to reflect changed in our code in the running container instantly.
# Once we done developing and put the container onto a server, it will run there without commands and without
# the bind mount. We might use other volumes to ensure that data survives, but will not use this bind mount, because
# if the container will run in production on a server, there is no connected source code which updates while it's 
# running. In production we always want to have a snapshot for our code, and that's why we keep the COPY in the
# Dockerfile, for the production. So we still want to be able to create images with snapshot of our code
# which we then could use to spin up production ready containers


62. WORKING WITH ENVIRONMENT VARIABLES & ".env" FILES

# Environment Variables - help us run one of the same container based on one of the same image in different modes,
# in different configurations. So to set containers more in a dynamic way when we build an image or when we run container

# We create new image with PORT as the environment variable on the Dockerfile
# docker build -t feedback-node:env .

# We creating container with the default value of our environment variable PORT that we set on the Dockerfile 
# that equal to 80. Important that the publish port in the command will be the same as the environment variable,
# so 80

# docker run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback -v "%cd%":/app:ro -v /app/temp -v /app/node_modules feedback-node:env

# We set port as environment variable in the commads and override our default port value to 8000 when we create container
# with --env PORT=8000

# docker run -d -p 3000:8000 --env PORT=8000 --rm --name feedback-app -v feedback:/app/feedback -v "%cd%":/app:ro -v /app/temp -v /app/node_modules feedback-node:env


# We set port as environment variable as before in the commands, and override our default port value to 8000 when we 
# create container with slightly different sintax -e PORT=8000. So, --env PORT=8000 equal to --env PORT=8000.
# We also can set like this multiple environment variable, not just one

# docker run -d -p 3000:8000 -e PORT=8000 --rm --name feedback-app -v feedback:/app/feedback -v "%cd%":/app:ro -v /app/temp -v /app/node_modules feedback-node:env

# Here we spcify a file that contains our environment variable (if we want), and pointing the location of the file,
# so ./.env => the same folder as we currently navigated in our terminal, and like this we always use the same command,
# and switch the value only in the file

# docker run -d -p 3000:8000 --env-file ./.env --rm --name feedback-app -v feedback:/app/feedback -v "%cd%":/app:ro -v /app/temp -v /app/node_modules feedback-node:env


 64. USING BUILD ARGUMENTS (ARG)

# Build Time Arguments - With those we can plug if different values into our Dockerfile or into our image when we build
# that image without having to hard code these values into the Dockerfile. In our image we use the 'ARG DEFAULT_PORT=80'
# as example to build-time-argument

# We build our image with our default value of the DEFAULT_PORT (80)
# docker build -t feedback-node:web-app .     

# We build our image with new DEFAULT_PORT (8000) that we set when we create the image - so we creating the same image
# with a new port without to changed the Dockerfile and create new imaged based on that
# docker build -t feedback-node:dev --build-arg DEFAULT_PORT=8000 . 


# SECTION-4

# The Dockerfile of this section for the node app

FROM node

WORKDIR /app

COPY package.json .

RUN npm install

COPY . .

CMD ["node", "app.js"]


72. CREATING A CONTAINER & COMMUNICATING TO THE WEB (WWW)

# *Sidenote* - The MongoDB database is not part of the container of node app. The installation of
# MongoDB is not part of the container. We have no code in the Dockerfile to install MongoDB

# Creating the image for our node app
# docker build -t favorites-node .     

# Creating container - Here we don't need to create volumes because there is not data that need to survived,
# we'll connect to another container and it will be our database

# docker run --name favorites -d --rm -p 3000:3000 favorites-node 


73. MAKING CONTAINER TO HOST COMMUNICATION WORK

# taken from app.js
# mongoose.connect(
# // we replace the localhost with host.docker.internal to connect our container to the local Database in our machine
#   'mongodb://host.docker.internal:27017/swfavorites',
#   { useNewUrlParser: true },
#   (err) => {
#     if (err) {
#       console.log(err);
#     } else {
#       app.listen(3000);
#     }
#   }
# );

# host.docker.internal - This special domain is rcongnized by Docker and it's translated to the IP address off
# our machine as seen from inside the Docker container. We also can use it anywhere we need a domain/URL, not
# just for MongoDB type of request, but also for an HTTP request if we had some web server runnig on our machine


74. CONTAINER TO CONTAINER COMMUNICATION: A BASIC SOLUTION

# We using MongoDB image from docker hub, so we don't need to create an image. This will create a new container
# based this MongoDB image, and this image will spin up a MongoDB database

# -d - for to not block the terminal, because by default it's attached mode 
# docker run -d --name mongodb mongo

# We alter the host.docker.internal to the IP of the continaer of MongoDB that contain the databse 

# We inspect the MongoDB database container to extract his IP, so we can connect to it from our node container.
# We looking 'IPAdress' under 'NetworkSettings', and then we connect to it from our node container (check in app.js)

# docker container inspect mongodb



# After we changed the IP address in app.js to the the IP of the MongoDB database container, we re-build the image,
# since we changed the code.

# docker build -t favorites-node .  


# We run our node container and connect to MongoDB database container

# docker run --name favorites -d --rm -p 3000:3000 favorites-node



75. INTRODUCING DOCKER NETWORKS: ELEGANT CONTAINER TO CONTAINER COMMUNICATION


# network - Within a Docker network, all containers can communicate with each other and IPs are automatically resolved


# docker network --help - To see our options/information about the network

# With network unlike volume, Docker will not automatically create them for us when we creating the container.
# We need to create them on our own, and then we give it a name. In our case it's 'favorites-net'
# docker network create favorites-net

# We can inspect all existing networks
# docker network ls

# docker rm NETWORK_NAME/S - Remove one or more networks
# docker network prune - Remove all unused networks

# We need to add the network communication and his name for to use the network, and then the network will be
# part of the favorites-net network, and then it will have the ability to communicate with the container/s that we want.
# docker run -d --name mongodb --network favorites-net mongo

# mongoose.connect(
#   // mongodb:27017 => The name of MongoDB database container. So if two containers are part of the same network,
#   // we can just put the other containers name. In our case it's 'mongodb', and we plug it as a domain.
#   // So the other Docker container's name will be translated by Docker to the IP address of that container,
#   // and the reson that it will work, it because that both of the containers part of the same network
#   'mongodb://mongodb:27017/swfavorites',
#   { useNewUrlParser: true },
#   (err) => {
#     if (err) {
#       console.log(err);
#     } else {
#       app.listen(3000);
#     }
#   }
# );

# We build the image again because we changed our code above in app.js
# docker build -t favorites-node .

# We create the our new favorites container from favorites-node and we connect it to the netowrk and his name.
# So the same network that we used in MongoDB container, so both of them will be part of the same network
# docker run --name favorites --network favorites-net -d --rm -p 3000:3000 favorites-node

# We inspected the logs of the favorites container and saw warning which proves that it works, because 
# it's not connection an error
# docker logs favorites

# *Sidenote* - When containers communicate/connect with each other, we don't need to publish any port when running
# that to be connected to container (The MongoDB container). So in our case when we run the MongoDB container, we don't 
# have the -p option, because -p only required if we plan connecting to something in that container from our local 
# machine, or from outside the container network. In our case the favorites container communicate with the MongoDB 
# container, not our local machine, and it will part of the same Docker network. So because we have container to 
# container connection, we don't need to publish the port, so when we created the container of MongoDB we didn't
# added the publish port (-p...) like the line below
# docker run -d --name mongodb --network favorites-net mongo



81. OUR TARGET APP & SETUP


# We create the MongoDB database container from Ducker Hub
# -p 27017:27017 => Since the backend API in not dockerized and the node API still talks to the database 
# when it connects - 'mongodb://localhost:27017/course-goals' as if it would be running on our local host machine,
# for that it work, we exposing our port to 27017 from our MongoDB container to our local machine, so our services can
# can connect to it trough our local machine, and with that we ensure that our locally running backend which is not
# dockerized yet, will still be able to talk to it
# docker run --name mongodb --rm -d -p 27017:27017 mongo 



83. DOCKERIZING THE NODE APP

FROM node

WORKDIR /app

COPY package.json .

RUN npm install

COPY . .

EXPOSE 80

CMD [ "node", "app.js" ]


# We build the backend image
# docker build -t goals-node . 

# We run a container based goals-node image
# docker run --name goals-backend --rm goals-node

# Here we get an error to connect to MongoDB, since we have MongoDB running in acontainer exposing its port, but in our 
# Dockerized backend application we reaching out to localhost - 'mongodb://localhost:27017/course-goals', and that now
# inside the container, means that we trying to access some other service on this port inside of that same backend 
# container, not on our host machine.
# So we use the special alternative domain or address - 'host.docker.internal' - That's special address/indentifier,
# which is translated to our real local host machine IP by Docker

# In short, Since we publish the MongoDB container on port 27017 on our local machine for our backend application
# before we Dockerized it and now we Dockrized the backend application, we cant communicate with our MongoDB container
# in localhost:27017, because we need to connect with that container on the local machine, since it's public there.
# So when we need to communicate between container/s and our local machine, we need to use 'host.docker.internal',
# unless we did inspect the IP of container that we want to connected to or that we use network

# Since we changed our code in the backend, we re-build the image
# docker build -t goals-node .

# We run this containe again and exposing his ports for the frontend can communicate with our backend container
# docker run --name goals-backend --rm -d -p 80:80 goals-node



84. MOVING THE REACT SPA INTO A CONTAINER

# From node => Since the frontend setup depends on Node. it's not Node application, but it uses Node to spin up the
# development server which serves this React application, and also Node is also used to optimize the React code which 
# we write, and to transorm it into code the browser understands 
FROM node

WORKDIR /app  

COPY package.json .

RUN npm install

COPY . .

EXPOSE 3000

CMD [ "npm", "start" ]


# We create our frontend docker image
# docker build -t goals-react .

# We create/run container based on goals-react image that we just build
# docker run --name goals-frontend --rm -d -p 3000:3000 goals-react

# Nowadays we don't need to add the "-it" because Docker fixed this problem, so we can ignore this line.

# The problem was that the container is not running anymore without the "-it" flag, because in old versions
# of Docke we nedded to run it in interactive mode by adding the "-it" option to the Docker run command, so
# we dont just start the container and be done with it ,but also let the container know that we want to able 
# to enter commands and interact with it. We don't using the interactive mode here, but React project is set
# up that if it doesn't receive this input trigger, it will stop the server   
# docker run --name goals-frontend --rm -d -p 3000:3000 -it goals-react 

# The right line command
# docker run --name goals-frontend --rm -d -p 3000:3000 goals-react 

# docker stop goals-frontend goals-backend mongodb  



85. Adding Docker Networks for Efficient Cross-Container Communication


# docker network ls

# We create a network
# docker network create goals-net 

# We running again the MongoDB container and put it into that network.
# docker run --name mongodb --rm -d --network goals-net mongo 

# Since we used in mongoose.connect() "host.docker.internal", we need to change it to "mongodb",
# because the MongoDB container not publishing right now the port 27017 => "-p 27017:27017" like before in our local 
# machine. So when we're published this port on our local machine, the backend container was connect to it with
# "host.docker.internal", since our MongoDB container was on our local machine published 27017 port.
# Now, we need to replace it with "mongodb", because we wanna connect it to the network, so the MongoDB container
# and the backend container will be able to connect on the network,  So for this we need to rebuild the image again.
# docker build -t goals-node .

# We run our backend container with the network being used.
# docker run --name goals-backend --rm -d --network goals-net goals-node

# We build the frontend container and connect to our localhost in our local machine (check in App.js)
# docker build -t goals-react .

# We create our goals-frontend container based on goal-react image and not using the network, since the part which
# runs in the container - the development server, doesn't care about the network and it doesn't interact with Node API
# or the Database, and the part that would interact with the API is not executed in the Docker environment
# docker run --name goals-frontend --rm -d -p 3000:3000 goals-react 

# *****We used this way at the end, since with the proxy we got an error*****
# The previous way we used before adding the "proxy" in package.json
# Here we run our goals-backend container, and publish it on port 80 on our local host machine, so that our 
# separate React application is able to talk to that
# docker run --name goals-backend --rm -d -p 80:80 --network goals-net goals-node

# The new and the right way
# Here we're not publishing the port 80, since we added the "proxy": "http://goals-backend:80" in package.json.
# "proxy": This setting is used to specify a backend server that the frontend should communicate, and setting a "proxy" 
# helps avoid CORS issues by forwarding requests to the backend.
# This solution is more secure, since only the frontend app (port 3000) is exposed and all backend calls are proxied 
# inside the container network.
# docker run --name goals-backend --rm -d --network goals-net goals-node

# docker stop goals-frontend goals-backend mongodb  



87. Adding Data Persistence to MongoDB with Volumes


# The problem with creating MongoDB contgainer like that, that when we start MongoDB container the data not persist
# docker run --name mongodb --rm -d --network goals-net mongo 

# We adding named volume that will store our data in the container on our local machine
# /data/db => The place where our database will save in our container - we know this from documentation -
# https://hub.docker.com/_/mongo. And now our data will survive from container to container
# docker run --name mongodb -v data:/data/db --rm -d --network goals-net mongo 


# For security we adding environment variables for allow access to containers to communicate with our MongoDB container, 
# only if they have name and password (we nedded to add ?authSource=admin also in the backend for it work) 
# in mongoose.connect()
# docker run --name mongodb -v data:/data/db --rm -d --network goals-net -e MONGO_INITDB_ROOT_USERNAME=uriel -e MONGO_INITDB_ROOT_PASSWORD=secret mongo

# Since we changed our code in the backend container, we re-build again the image
# docker build -t goals-node .

# ***We run our application by running all the containers.***

# docker run --name goals-backend --rm -d -p 80:80 --network goals-net goals-node

# docker run --name goals-frontend --rm -d -p 3000:3000 goals-react

# docker run --name mongodb -v data:/data/db --rm -d --network goals-net mongo 

# We stop them
# docker stop goals-frontend goals-backend mongodb  



88. Volumes, Bind Mounts & Polishing for the NodeJS Container


# ***We create our container with a bind mount, named volume and anonymous volume***

# -v "%cd%":/app => Bind Moutn - So all our changes in our local machine in our root directory in the backend
# (since the Dockerfile there) will be reflect in the continaer in the working/root directory (app folder).

# -v logs:/app/logs => Named volume - So the logs in the backend will save from container to container,
# and since the path of the named volume longer then the path of the bind mount, he will not be affected from
# our changes in the local machine.

# -v /app/node_modules => Anonymous Volume - So that if we change something in the local machine, the node_modules
# will taken from the container - so from the snapshot when the image created (and run npm install), and  not from
# the local machine, since we mabe not have the node_modules there, or that they different/outdated.

# docker run --name goals-backend -v "%cd%":/app -v logs:/app/logs -v /app/node_modules -d --rm -p 80:80 --network goals-net goals-node

# Here we added the nodemon in package.json, and We needed to add -L in the scripts for nodemon work in windows
#  because we using WSL 2
# "start": "nodemon --legacy-watch app.js"
# Solution 2: "start": "nodemon -L server.js" 

# Since we changed our code - the package.json and the Dockerfile, we re-build the image again
# docker build -t goals-node .

# ***We run our applicatin with all the containers***

# docker run --name mongodb -v data:/data/db --rm -d --network goals-net mongo 

# docker run --name goals-frontend --rm -d -p 3000:3000 goals-react

# docker run --name goals-backend -v "%cd%":/app -v logs:/app/logs -v /app/node_modules -d --rm -p 80:80 --network goals-net goals-node

# For stop them
# docker stop goals-frontend goals-backend mongodb  


# We add environment variables in the Dockerfile:

FROM node

WORKDIR /app

COPY package.json .

RUN npm install

COPY . .

EXPOSE 80

ENV MONGODB_USERNAME=root
ENV MONGODB_PASSWORD=secret


CMD [ "npm", "start" ]


# We access them from our backend - `mongodb://${process.env.MONGODB_USERNAME}:${process.env.MONGODB_PASSWORD}@mongodb:27017/course-goals?authSource=admin`,

# Since we changed our code in the Dockerfile and app.js, we re-build the image again
# docker build -t goals-node .

# ***We run our applicatin with all the containers***

# docker run --name mongodb -v data:/data/db --rm -d --network goals-net mongo 

# docker run --name goals-frontend --rm -d -p 3000:3000 goals-react

# We run the container, and provide our name as environment variable
# docker run --name goals-backend -v "%cd%":/app -v logs:/app/logs -v /app/node_modules -e MONGODB_USERNAME=uriel -d --rm -p 80:80 --network goals-net goals-node

# After that we added the .dockerignore, so when we create the image we'll not add to the image things like -
# node_modules, git, some private folders etc..



89. Live Source Code Updates for the React Container (with Bind Mounts)


 
# We add Bind Mount to bind the src folder in the container to the src folder in our local machine in the frontend,
# so that whenever we change the code in src folder in our local machine, it will change/reflected on the src folder
# in the continer

# -e WATCHPACK_POLLING=true - Environment Variable for live relode. We can also add this in package.json, so:
# change the start script => "start": "WATCHPACK_POLLING=true react-scripts start", and re-build the image.

# ***Sidenote*** - Since we only use the Bind Mount in the src folder in our local machine to src folder in the 
# container, we don't need to add anonymous volume for to take the node_modules from the container, so from the 
# first snapshot when we created the image, because the node_modules is not in the src folder, but his brothers

# -it - So we can interaction with the container (we also could exist with ctrl + c in interaction mode) 

# docker run -e WATCHPACK_POLLING=true -v "%cd%"/src:/app/src --name goals-frontend --rm -p 3000:3000 -it goals-react

# docker run --name mongodb -v data:/data/db --rm -d --network goals-net mongo 

# docker run --name goals-backend -v "%cd%":/app -v logs:/app/logs -v /app/node_modules -e MONGODB_USERNAME=uriel -d --rm -p 80:80 --network goals-net goals-node

# docker stop goals-frontend goals-backend mongodb  

# After that we added the .dockerignore, so when we create the image we'll not add to the image things like -
# node_modules, git, some private folders etc..



94. Creating a Compose File

# link for details on Compose file versions and upgrading - https://docker-docs.uclv.cu/compose/compose-file/compose-versioning/



95. Diving into the Compose File Configuration


# ***Sidenote*** when we use compose, we don't need to add --rm, because it will deleted automatically
# and for detached mode we can specify it when we start all services together 



97. Docker Compose Up & Down


# It will start all the services that defined in the compose file, and it will not just start the containers, it
# also will pull and build all the images that might be required.
# docker-compose up 

# If we want to start with detached mode, since the default is attached mode
# docker-compose up -d 

# Stop all the services and remove all containers and the default network it created and so on..
# docker-compose down 

# For delete volumes. The reason that in deleted when we stop the services, it's because typically we do want
# to persist data with volumes.
# docker-compose down -v



98. Working with Multiple Containers


# The version of the Docker Compose document, so the Docker compose specification wa wanna use and the version
# we defined here has an effect on the features we can use in this compose file
version: '3.8'
services:
  mongodb:
    # image - It can be image name which will be looked up locally and in the Docker hub repository, it can be a full URL
    # to another repository where this image stored, it could be our own repository, or a custom image which we build
    # on our system like 'goals-node' that we builted
    image: 'mongo'
    volumes:
      # /data/db => The place where our database will save in our container - we know this from documentation
      - data:/data/db
    # environment:
        # We can specify our environments with a dash and colon, or with equal sign. So dash is more for we specify a 
        # single value, with a colon it creates a yaml object, so we dont need dashes
    #   MONGO_INITDB_ROOT_USERNAME: uriel
    #   MONGO_INITDB_ROOT_PASSWORD: secret
    #   # - MONGO_INITDB_ROOT_USERNAME=uriel
    env_file:
    # ./ - Means that it will start in the same direcotry as the docker-compose is in    
      - ./env/mongo.env
    # We can add network by adding the networks key and specify all the networks this container should belong to,
    # but many cases, we don't need, since Docker Compose automatically create a new environment for all the services
    # specified in the compose file, and it will add all the services to that network out of the box.
    # So all these services which are defined in one and the same compose file, will already be part of one and the 
    # same network that was created for them by Docker.  
    # networks:
     # Example of network if we wanna use our own network.
     # *Sidenote* - If we using like this in network, the MongoDB service will create also the default network but also
     # this special network that we specify here
     # -golas-net
  backend:
    # If we want to specify existing image 
    # image: 'goals-node'

    # We also give Docker Compose all the information it needs to build an image
    # First way:
    # ./backend - The build want to know where to find the Dockerfile that should be built. In its simplest form,
    # we can use a relative path, for example: './' - so we saying i'ts in the same folder as the docker-compose.yaml file
    # or to say it's in the './backend' folder in this case
    build: ./backend
    # Second way:
    # The longer form to build is that we have an object with nested keys
    # build: 
      #***Sidenote about the context*** - It can be the path to our Dockerfile, but it will also be the place where the
      # the Dockerfile is built thereafter, so where the image will be generated. So our context should be set to a 
      # folder which includes everything the Dockerfile might be referring to. So if in our Dockerfile we're copying some
      # folder into the image, the context must be set to a folder that includes that to be copied folder. In this case
      # this would be the case - the Dockerfile in the backend folder only refers to other folders which are included in
      # the backend folder. So setting the context to backend would be okay, but if our Dockerfile would be in some other
      # nested folder and needs access to folders outside of that nested folder, then our context should be set to a
      # higher level folder. 
      # context - The path to the folder that holds the Dockerfile
    #   context: ./backend
      # dockerfile - dockerfile key which specifies the file name. If our file is named Dockerfile, there is no need to 
      # use this longer form, but if we named it differently like Dockerfile-dev, then this is how we could tell Docker
      # Compose which Dockerfile to use
    #   dockerfile: Dockerfile-dev
    #   args:
    #     some-arg: 1
    ports:
      # We could specify here multiple ports
      # The second port is the container internal port, and the first is the port on our local machine where it will be 
      # exposed.
      - '80:80'
    volumes:
      # Our named volume
      - logs:/app/logs
       # Our bind mount volume
       # With Docker Compose we're allowed to use a relative path - relative from the docker-compose.yaml, not like the 
       # Docker run that we needed the absolute path
      - ./backend:/app
       # Our anonymous volume
      - /app/node_modules
    env_file:
    # The file where our environment variables
      - ./env/backend.env  
    # depends_on - When we create and launch multiple services/containers at the same time, sometimes one container
    # might depend on another container to be up and running already. So for example here, our backend depends on 
    # mongodb container being up and runing already, because our backend container wants to connect to MongoDB.
    # So we let compose know  that it should first bring up mongodb and only once that is up and runing, it should 
    # create this backend container. In our case it only depends on mongodb, but it could be depend on multiple services
    depends_on: 
      - mongodb      
  # frontend:

# For named volumes we should add key next to services, so on the same level as services, and any named volumes we using
# in our services have to be listed here - in this case 'data', and we simply add this with a colon.
# *Additional note* - If we use the same volume name in different services, the volume will be shared. So different 
# containers can use the same volume, the same folder on our hosting machine
volumes:
  data:
  logs:
















